{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the Carbon Footprint of the Neural Network Training\n",
    "In this tutorial we will demonstrate how to use the **codecarbon** Python package to **estimate** the carbon footprint produced by training a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "We will build a Neural Network using the Keras Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 00:10:51.287837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000 # vocabulary size\n",
    "maxlen = 256 # The length of every input sequence\n",
    "\n",
    "embedding_size = 64  # the dimension of the embeddings\n",
    "dropout = 0.1\n",
    "filters = 32  # num of 1D convolution filters (output num channels)\n",
    "kernel_size = 5  # 1D convolution kernel width\n",
    "pool_size = 4\n",
    "lstm_output_size = 16  # number of LSTM units\n",
    "batch_size = 32\n",
    "\n",
    "epochs = 100  # number of training epochs\n",
    "test_batch_size = 32  # batch size for testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "We will load the IMDb sentiment analysis dataset directly from the Keras dataset module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 256)\n",
      "x_test shape: (25000, 256)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "We will build a simple model typically used for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    metrics: list,\n",
    "    vocab_size: int,\n",
    "    embedding_dim: int,\n",
    "    dropout: float,\n",
    "    filters: int,\n",
    "    kernel_size: int,\n",
    "    pool_size: int,\n",
    "    lstm_output_size: int,\n",
    ") -> Sequential:\n",
    "    \"\"\"Builds a simple Keras model.\n",
    "\n",
    "    Args:\n",
    "        metrics (list): list of metrics to report\n",
    "\n",
    "        vocab_size (int): nunmber of tokens in the vocabulary.\n",
    "        It determines the embedding look-up yable size.\n",
    "\n",
    "        embedding_dim (int): the dimensionbality of the\n",
    "        embedding vectors.\n",
    "\n",
    "        dropout (float): dropout rate after the embedding layer.\n",
    "\n",
    "        filters (int): number of output channels of the 1D convolution.\n",
    "\n",
    "        kernel_size (int): the kernel size (width) of the 1D convolution.\n",
    "\n",
    "        pool_size (int): the size (width) of the 1D max pooling layer.\n",
    "\n",
    "        lstm_output_size (int): the dimensionality of the output vectors\n",
    "        from the final bi-LSTM layer.\n",
    "\n",
    "    Returns:\n",
    "        Sequential: compiled Keras sequential model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "        Dropout(dropout),\n",
    "        Conv1D(filters, kernel_size, padding='valid', activation='relu'),\n",
    "        MaxPooling1D(pool_size=pool_size),\n",
    "        Bidirectional(LSTM(lstm_output_size), merge_mode='ave'),\n",
    "        Dense(1),\n",
    "        Activation('sigmoid'),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We write a simple function to train the model. As we don't use cross-validation, we will use the training data as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = make_model(\n",
    "        metrics=[\"accuracy\"],\n",
    "        vocab_size=max_features,\n",
    "        embedding_dim=embedding_size,\n",
    "        dropout=dropout,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        pool_size=pool_size,\n",
    "        lstm_output_size=lstm_output_size,\n",
    "    )\n",
    "\n",
    "    h = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the CO2 emissions\n",
    "This is where we start tracking the CO2 emissions. It is as simple as timing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = EmissionsTracker(project_name=\"imbd_sentiment_classification\")\n",
    "tracker.start()\n",
    "train_model()\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amusive-blogging-n-coding-u72LGqej-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 15:49:01) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "335da5445bb9a0de01ba9c6ab14e4471a845550dd1888d0179898dba3954f3be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
